---
title: "Proyecto Final - Estad√≠stica Aplicada III"
subtitle: "An√°lisis Multivariado del Dataset Breast Cancer Wisconsin"
author: "Tu Nombre"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    latex_engine: xelatex
geometry: margin=1in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  fig.pos = 'H',
  fig.width = 8,
  fig.height = 6
)
```

\newpage

# Introducci√≥n

Este proyecto aplica t√©cnicas de **Estad√≠stica Multivariada** al dataset **Breast Cancer Wisconsin (Diagnostic)** con el objetivo de:

1. Validar supuestos de normalidad multivariada
2. Explorar estructuras de covarianza (PCA, FA, CCA)
3. Comparar modelos de clasificaci√≥n supervisada (LDA manual, Regresi√≥n Log√≠stica, Random Forest)

El dataset contiene 569 observaciones de tumores mamarios con 30 caracter√≠sticas cuantitativas derivadas de im√°genes digitalizadas de aspiraci√≥n con aguja fina (FNA). La variable respuesta es el diagn√≥stico: **Maligno (M)** o **Benigno (B)**.

---

# Configuraci√≥n y Preprocesamiento

## Carga de Librer√≠as

```{r libraries}
# Librer√≠as para an√°lisis multivariado
library(tidyverse)    # Manipulaci√≥n y visualizaci√≥n de datos
library(MVN)          # Tests de normalidad multivariada
library(psych)        # Factor Analysis
library(CCA)          # Canonical Correlation Analysis
library(MASS)         # LDA

# Librer√≠as para Machine Learning
library(caret)        # Partici√≥n de datos y m√©tricas
library(randomForest) # Random Forest

# Librer√≠as para presentaci√≥n
library(knitr)        # Generaci√≥n de reportes
library(kableExtra)   # Tablas bonitas
```

## Carga y Preparaci√≥n de Datos

```{r load_data}
# Definir nombres de columnas seg√∫n documentaci√≥n
feature_names <- c(
  "radius", "texture", "perimeter", "area", "smoothness",
  "compactness", "concavity", "concave_points", "symmetry", "fractal_dimension"
)

column_names <- c(
  "ID",
  "Diagnosis",
  paste0(feature_names, "_mean"),
  paste0(feature_names, "_se"),
  paste0(feature_names, "_worst")
)

# Cargar datos
data_raw <- read.csv("data/wdbc.data", header = FALSE, col.names = column_names)

# Eliminar ID (no aporta informaci√≥n predictiva)
data <- data_raw %>% select(-ID)

# Convertir Diagnosis a factor
data$Diagnosis <- factor(data$Diagnosis, levels = c("B", "M"))

# Mostrar dimensiones y primeras filas
cat("Dimensiones del dataset:", dim(data)[1], "filas x", dim(data)[2], "columnas\n")
cat("\nDistribuci√≥n de clases:\n")
print(table(data$Diagnosis))
```

```{r preview_data}
# Vista previa de los datos
head(data, 5) %>%
  kable(caption = "Primeras 5 observaciones del dataset") %>%
  kable_styling(latex_options = c("scale_down", "HOLD_position"))
```

## Partici√≥n de Datos: Train (70%) y Test (30%)

**Justificaci√≥n:** Para evitar sobreajuste, entrenaremos todos los modelos sobre el conjunto de entrenamiento y evaluaremos el desempe√±o final sobre el conjunto de prueba independiente.

```{r train_test_split}
# Fijar semilla para reproducibilidad
set.seed(123)

# Crear √≠ndices de partici√≥n estratificada (mantiene proporci√≥n de clases)
train_index <- createDataPartition(data$Diagnosis, p = 0.7, list = FALSE)

# Dividir datos
data_train <- data[train_index, ]
data_test <- data[-train_index, ]

cat("Observaciones en Train:", nrow(data_train), "\n")
cat("Observaciones en Test:", nrow(data_test), "\n\n")

cat("Distribuci√≥n de clases en Train:\n")
print(table(data_train$Diagnosis))
cat("\nDistribuci√≥n de clases en Test:\n")
print(table(data_test$Diagnosis))
```

## Estandarizaci√≥n de Variables

**Nota:** La estandarizaci√≥n es cr√≠tica para PCA, LDA y otros m√©todos sensibles a la escala.

```{r standardization}
# Separar variables predictoras y respuesta en Train
X_train <- data_train %>% select(-Diagnosis)
y_train <- data_train$Diagnosis

# Estandarizar datos de entrenamiento (media=0, sd=1)
X_train_scaled <- scale(X_train)

# Guardar par√°metros de estandarizaci√≥n para aplicar a Test
scaling_params <- list(
  center = attr(X_train_scaled, "scaled:center"),
  scale = attr(X_train_scaled, "scaled:scale")
)

# Estandarizar datos de prueba usando par√°metros de Train
X_test <- data_test %>% select(-Diagnosis)
y_test <- data_test$Diagnosis

X_test_scaled <- scale(X_test,
                       center = scaling_params$center,
                       scale = scaling_params$scale)

cat("Verificaci√≥n de estandarizaci√≥n en Train:\n")
cat("Media de la primera variable:", round(mean(X_train_scaled[,1]), 6), "\n")
cat("Desviaci√≥n est√°ndar de la primera variable:", round(sd(X_train_scaled[,1]), 6), "\n")
```

\newpage

# Punto Adicional: Tests de Normalidad Multivariada

La normalidad multivariada es un **supuesto clave para LDA**. Evaluaremos este supuesto mediante dos tests:

1. **Test de Mardia**: Eval√∫a asimetr√≠a (skewness) y curtosis (kurtosis) multivariadas
2. **Test de Royston**: Extensi√≥n multivariada del test de Shapiro-Wilk

## Test de Mardia

El test de Mardia eval√∫a:
- $H_0$: Los datos provienen de una distribuci√≥n normal multivariada
- $H_a$: Los datos no son normales multivariados

```{r mardia_test}
# Aplicar test de Mardia sobre datos de entrenamiento
mardia_result <- mvn(data = X_train_scaled, mvnTest = "mardia", multivariatePlot = "none")

# Extraer resultados
mardia_stats <- mardia_result$multivariateNormality

mardia_stats %>%
  kable(caption = "Test de Mardia para Normalidad Multivariada",
        digits = 4,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")
```

**Interpretaci√≥n:**

- **Skewness (Asimetr√≠a):** $p$-value `r mardia_stats$`p value`[1]`. Si $p < 0.05$, rechazamos normalidad por asimetr√≠a.
- **Kurtosis (Curtosis):** $p$-value `r mardia_stats$`p value`[2]`. Si $p < 0.05$, rechazamos normalidad por curtosis excesiva.

```{r mardia_interpretation, echo=FALSE}
skew_p <- mardia_stats$`p value`[1]
kurt_p <- mardia_stats$`p value`[2]

if(skew_p < 0.05 | kurt_p < 0.05) {
  cat("‚ùå Los datos NO cumplen el supuesto de normalidad multivariada seg√∫n Mardia.\n")
} else {
  cat("‚úì Los datos cumplen el supuesto de normalidad multivariada seg√∫n Mardia.\n")
}
```

## Test de Royston

```{r royston_test}
# Aplicar test de Royston
royston_result <- mvn(data = X_train_scaled, mvnTest = "royston", multivariatePlot = "none")

royston_stats <- royston_result$multivariateNormality

royston_stats %>%
  kable(caption = "Test de Royston para Normalidad Multivariada",
        digits = 4,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")
```

**Interpretaci√≥n:**

- $H$-statistic: `r royston_stats$H`
- $p$-value: `r royston_stats$`p value``

```{r royston_interpretation, echo=FALSE}
roy_p <- royston_stats$`p value`

if(roy_p < 0.05) {
  cat("‚ùå Los datos NO cumplen el supuesto de normalidad multivariada seg√∫n Royston.\n")
} else {
  cat("‚úì Los datos cumplen el supuesto de normalidad multivariada seg√∫n Royston.\n")
}
```

## Conclusi√≥n sobre Normalidad

```{r normality_conclusion, echo=FALSE}
cat("\nüîç CONCLUSI√ìN GENERAL:\n")
cat("A pesar de que los tests rechazan la normalidad multivariada estricta,\n")
cat("LDA es robusto ante desviaciones moderadas de normalidad, especialmente\n")
cat("con tama√±os de muestra grandes (n = ", nrow(X_train_scaled), ").\n", sep="")
cat("Procederemos con LDA reconociendo esta limitaci√≥n.\n")
```

\newpage

# Estructuras de Covarianza (An√°lisis No Supervisado)

## PCA: An√°lisis de Componentes Principales

### Implementaci√≥n Manual con √Ålgebra Matricial

El objetivo de PCA es encontrar direcciones de m√°xima varianza mediante la descomposici√≥n espectral de la matriz de covarianza.

**Matem√°tica:**

Dada la matriz de covarianza $\mathbf{\Sigma} = \frac{1}{n-1}\mathbf{X}^T\mathbf{X}$ (con datos centrados), encontramos los eigenvectores $\mathbf{v}_i$ y eigenvalores $\lambda_i$ tal que:

$$\mathbf{\Sigma}\mathbf{v}_i = \lambda_i\mathbf{v}_i$$

Los **componentes principales** son las proyecciones:

$$\mathbf{Z} = \mathbf{X}\mathbf{V}$$

donde $\mathbf{V}$ es la matriz de eigenvectores.

```{r pca_manual}
# 1. Calcular matriz de covarianza
cov_matrix <- cov(X_train_scaled)

# 2. Descomposici√≥n espectral (eigenvalues y eigenvectors)
eigen_decomp <- eigen(cov_matrix)

# 3. Extraer eigenvalores y eigenvectores
eigenvalues <- eigen_decomp$values
eigenvectors <- eigen_decomp$vectors

# 4. Calcular scores (proyecci√≥n de datos sobre eigenvectores)
pca_scores_manual <- X_train_scaled %*% eigenvectors

# 5. Calcular proporci√≥n de varianza explicada
prop_var_explained <- eigenvalues / sum(eigenvalues)
cum_var_explained <- cumsum(prop_var_explained)

# Crear tabla de varianza explicada
variance_table <- data.frame(
  PC = paste0("PC", 1:10),
  Eigenvalue = eigenvalues[1:10],
  Proportion = prop_var_explained[1:10],
  Cumulative = cum_var_explained[1:10]
)

variance_table %>%
  kable(caption = "Varianza Explicada por los Primeros 10 Componentes Principales (Manual)",
        digits = 4,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")
```

**Interpretaci√≥n:** Los primeros `r sum(cum_var_explained < 0.95)` componentes principales explican el 95% de la varianza total.

### Comparaci√≥n con `prcomp()`

```{r pca_prcomp}
# PCA usando funci√≥n nativa de R
pca_prcomp <- prcomp(X_train_scaled, center = FALSE, scale. = FALSE)

# Comparar primeros 5 eigenvalores
comparison <- data.frame(
  Component = paste0("PC", 1:5),
  Manual = eigenvalues[1:5],
  prcomp = (pca_prcomp$sdev^2)[1:5],
  Diferencia = abs(eigenvalues[1:5] - (pca_prcomp$sdev^2)[1:5])
)

comparison %>%
  kable(caption = "Comparaci√≥n de Eigenvalores: Manual vs. prcomp()",
        digits = 6,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")

cat("\n‚úì Los resultados son id√©nticos, validando nuestra implementaci√≥n manual.\n")
```

### Visualizaciones de PCA

```{r scree_plot, fig.cap="Scree Plot: Varianza Explicada por Componente"}
# Scree plot
scree_data <- data.frame(
  PC = 1:length(eigenvalues),
  Variance = eigenvalues,
  Proportion = prop_var_explained
)

ggplot(scree_data[1:15,], aes(x = PC, y = Proportion)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "steelblue", size = 3) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  labs(title = "Scree Plot",
       subtitle = "Proporci√≥n de Varianza Explicada por Componente Principal",
       x = "Componente Principal",
       y = "Proporci√≥n de Varianza") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))
```

```{r biplot, fig.cap="Biplot: PC1 vs PC2 con Clases de Diagn√≥stico"}
# Crear dataframe para biplot
biplot_data <- data.frame(
  PC1 = pca_scores_manual[, 1],
  PC2 = pca_scores_manual[, 2],
  Diagnosis = y_train
)

# Biplot de scores coloreados por clase
ggplot(biplot_data, aes(x = PC1, y = PC2, color = Diagnosis)) +
  geom_point(alpha = 0.6, size = 2.5) +
  scale_color_manual(values = c("B" = "#00BA38", "M" = "#F8766D"),
                     labels = c("Benigno", "Maligno")) +
  labs(title = "Biplot: Primeros Dos Componentes Principales",
       subtitle = paste0("PC1 (", round(prop_var_explained[1]*100, 1), "%) vs PC2 (",
                        round(prop_var_explained[2]*100, 1), "%)"),
       x = paste0("PC1 (", round(prop_var_explained[1]*100, 1), "% varianza)"),
       y = paste0("PC2 (", round(prop_var_explained[2]*100, 1), "% varianza)")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom")
```

**Interpretaci√≥n del Biplot:** Se observa una separaci√≥n parcial entre tumores benignos y malignos en el espacio de los dos primeros componentes principales, lo que sugiere que las caracter√≠sticas originales contienen informaci√≥n discriminante.

\newpage

## Factor Analysis (FA)

El An√°lisis Factorial busca **variables latentes (factores)** que expliquen las correlaciones entre las variables observadas. A diferencia de PCA, FA modela la varianza compartida (comunalidades) y separa la varianza √∫nica (unicidades).

```{r factor_analysis}
# Determinar n√∫mero √≥ptimo de factores usando an√°lisis paralelo
# fa.parallel(X_train_scaled, fa = "fa") # Descomentar para visualizar

# Aplicar FA con 5 factores y rotaci√≥n Varimax
fa_result <- fa(X_train_scaled, nfactors = 5, rotate = "varimax", fm = "ml")

# Mostrar loadings (cargas factoriales)
print(fa_result$loadings, cutoff = 0.3)

# Tabla de loadings principales
loadings_matrix <- as.data.frame(unclass(fa_result$loadings))
loadings_matrix$Variable <- rownames(loadings_matrix)
loadings_matrix <- loadings_matrix[, c("Variable", paste0("ML", 1:5))]

loadings_matrix %>%
  head(10) %>%
  kable(caption = "Cargas Factoriales (primeras 10 variables)",
        digits = 3,
        booktabs = TRUE) %>%
  kable_styling(latex_options = c("scale_down", "HOLD_position"))
```

**Interpretaci√≥n de Factores:**

```{r fa_interpretation}
# Varianza explicada por cada factor
var_explained_fa <- fa_result$Vaccounted

var_explained_fa %>%
  kable(caption = "Varianza Explicada por Factores (FA)",
        digits = 3,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")

cat("\nüîç Factor 1 probablemente captura caracter√≠sticas de TAMA√ëO del tumor (radius, area, perimeter).\n")
cat("üîç Factor 2 puede relacionarse con TEXTURA Y FORMA (texture, smoothness, symmetry).\n")
cat("üîç Los factores restantes capturan aspectos m√°s espec√≠ficos de la morfolog√≠a tumoral.\n")
```

\newpage

## Canonical Correlation Analysis (CCA)

CCA identifica las **combinaciones lineales de dos conjuntos de variables** que tienen la m√°xima correlaci√≥n entre s√≠.

**Conjuntos:**
- **Set X**: Variables "Mean" (caracter√≠sticas promedio)
- **Set Y**: Variables "Worst" (valores m√°ximos/peores)

**Pregunta de investigaci√≥n:** ¬øLas caracter√≠sticas promedio est√°n fuertemente correlacionadas con los valores extremos?

```{r cca_analysis}
# Seleccionar variables Mean (columnas 1-10) y Worst (columnas 21-30)
X_mean <- X_train_scaled[, 1:10]
X_worst <- X_train_scaled[, 21:30]

# Calcular correlaciones can√≥nicas
cca_result <- cc(X_mean, X_worst)

# Correlaciones can√≥nicas
cat("Correlaciones Can√≥nicas:\n")
print(round(cca_result$cor, 4))

# Tabla de correlaciones can√≥nicas
cca_table <- data.frame(
  Canonical_Variable = paste0("CV", 1:10),
  Correlation = cca_result$cor
)

cca_table %>%
  kable(caption = "Correlaciones Can√≥nicas entre Variables Mean y Worst",
        digits = 4,
        booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")
```

**Interpretaci√≥n:**

La primera correlaci√≥n can√≥nica (`r round(cca_result$cor[1], 3)`) indica una **fuerte asociaci√≥n lineal** entre las caracter√≠sticas promedio y los valores extremos. Esto es esperado, ya que tumores con medidas promedio altas tienden a tener valores m√°ximos altos tambi√©n.

\newpage

# Modelos de Clasificaci√≥n Supervisada

Compararemos tres enfoques:

1. **LDA Manual**: Implementaci√≥n desde cero con √°lgebra matricial
2. **Regresi√≥n Log√≠stica**: Modelo lineal generalizado est√°ndar
3. **Random Forest**: Modelo no lineal basado en ensambles de √°rboles

---

## LDA: Linear Discriminant Analysis (Implementaci√≥n Manual)

LDA busca la proyecci√≥n que **maximiza la separaci√≥n entre clases** minimizando la varianza intra-clase.

### Fundamento Matem√°tico

Dado:
- $\mathbf{\bar{x}}_M$, $\mathbf{\bar{x}}_B$: vectores de medias para clases Maligno y Benigno
- $\mathbf{S}_W$: matriz de covarianza intra-clase (within-class scatter)
- $\mathbf{S}_B$: matriz de covarianza inter-clase (between-class scatter)

El vector discriminante √≥ptimo $\mathbf{w}$ es el eigenvector asociado al eigenvalor m√°ximo de:

$$\mathbf{S}_W^{-1}\mathbf{S}_B\mathbf{w} = \lambda\mathbf{w}$$

Para dos clases, la soluci√≥n es:

$$\mathbf{w} \propto \mathbf{S}_W^{-1}(\mathbf{\bar{x}}_M - \mathbf{\bar{x}}_B)$$

### Implementaci√≥n Manual

```{r lda_manual}
# 1. Separar datos por clase
X_train_M <- X_train_scaled[y_train == "M", ]
X_train_B <- X_train_scaled[y_train == "B", ]

n_M <- nrow(X_train_M)
n_B <- nrow(X_train_B)

cat("Observaciones de clase Maligno:", n_M, "\n")
cat("Observaciones de clase Benigno:", n_B, "\n\n")

# 2. Calcular vectores de medias por clase
mean_M <- colMeans(X_train_M)
mean_B <- colMeans(X_train_B)

# 3. Calcular matriz de covarianza within-class (S_W)
# S_W = (n_M - 1) * Cov(X_M) + (n_B - 1) * Cov(X_B)
S_M <- cov(X_train_M)
S_B_class <- cov(X_train_B)

S_W <- ((n_M - 1) * S_M + (n_B - 1) * S_B_class) / (n_M + n_B - 2)

# 4. Calcular matriz de covarianza between-class (S_B)
# Para dos clases: S_B = (mean_M - mean_B) %*% t(mean_M - mean_B)
mean_diff <- mean_M - mean_B
S_B_between <- mean_diff %*% t(mean_diff)

# 5. Resolver eigen-problema: S_W^(-1) * S_B
eigen_lda <- eigen(solve(S_W) %*% S_B_between)

# 6. Vector discriminante (primer eigenvector)
w_lda <- eigen_lda$vectors[, 1]

cat("Vector discriminante LDA (primeras 5 componentes):\n")
print(round(w_lda[1:5], 4))

# 7. Proyectar datos de entrenamiento y prueba
train_projection <- X_train_scaled %*% w_lda
test_projection <- X_test_scaled %*% w_lda

# 8. Calcular punto de corte (threshold) usando promedio de proyecciones de clases
threshold_M <- mean(train_projection[y_train == "M"])
threshold_B <- mean(train_projection[y_train == "B"])
threshold <- (threshold_M + threshold_B) / 2

cat("\nThreshold de clasificaci√≥n:", round(threshold, 4), "\n")

# 9. Clasificar datos de TEST
predictions_lda_manual <- ifelse(test_projection > threshold, "M", "B")
predictions_lda_manual <- factor(predictions_lda_manual, levels = c("B", "M"))
```

### Evaluaci√≥n del LDA Manual

```{r lda_manual_eval}
# Matriz de confusi√≥n
conf_matrix_lda <- confusionMatrix(predictions_lda_manual, y_test, positive = "M")

print(conf_matrix_lda$table)

# Extraer m√©tricas
accuracy_lda <- conf_matrix_lda$overall["Accuracy"]
sensitivity_lda <- conf_matrix_lda$byClass["Sensitivity"]
specificity_lda <- conf_matrix_lda$byClass["Specificity"]

cat("\nüìä M√âTRICAS LDA MANUAL:\n")
cat("Accuracy:", round(accuracy_lda, 4), "\n")
cat("Sensibilidad (Recall):", round(sensitivity_lda, 4), "\n")
cat("Especificidad:", round(specificity_lda, 4), "\n")
```

### Visualizaci√≥n de Proyecci√≥n LDA

```{r lda_visualization, fig.cap="Distribuci√≥n de Proyecciones LDA por Clase"}
# Crear dataframe para visualizaci√≥n
lda_viz_data <- data.frame(
  Projection = c(train_projection, test_projection),
  Diagnosis = c(as.character(y_train), as.character(y_test)),
  Dataset = c(rep("Train", length(train_projection)),
              rep("Test", length(test_projection)))
)

ggplot(lda_viz_data, aes(x = Projection, fill = Diagnosis)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = threshold, linetype = "dashed", color = "black", size = 1) +
  scale_fill_manual(values = c("B" = "#00BA38", "M" = "#F8766D"),
                    labels = c("Benigno", "Maligno")) +
  labs(title = "Distribuci√≥n de Proyecciones LDA",
       subtitle = "L√≠nea punteada indica threshold de clasificaci√≥n",
       x = "Proyecci√≥n LDA",
       y = "Densidad") +
  facet_wrap(~ Dataset) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom")
```

\newpage

## Regresi√≥n Log√≠stica

La regresi√≥n log√≠stica modela la probabilidad de la clase Maligno como:

$$P(Y = M | \mathbf{X}) = \frac{1}{1 + e^{-(\beta_0 + \boldsymbol{\beta}^T\mathbf{X})}}$$

```{r logistic_regression}
# Crear dataframe para modelo
train_data_glm <- data.frame(Diagnosis = y_train, X_train_scaled)

# Entrenar modelo de regresi√≥n log√≠stica
logit_model <- glm(Diagnosis ~ ., data = train_data_glm, family = binomial(link = "logit"))

# Resumen del modelo (opcional, puede ser extenso)
# summary(logit_model)

# Predecir en datos de TEST
test_data_glm <- data.frame(X_test_scaled)
predictions_logit_prob <- predict(logit_model, newdata = test_data_glm, type = "response")

# Clasificar usando threshold = 0.5
predictions_logit <- ifelse(predictions_logit_prob > 0.5, "M", "B")
predictions_logit <- factor(predictions_logit, levels = c("B", "M"))

# Matriz de confusi√≥n
conf_matrix_logit <- confusionMatrix(predictions_logit, y_test, positive = "M")

print(conf_matrix_logit$table)

# Extraer m√©tricas
accuracy_logit <- conf_matrix_logit$overall["Accuracy"]
sensitivity_logit <- conf_matrix_logit$byClass["Sensitivity"]
specificity_logit <- conf_matrix_logit$byClass["Specificity"]

cat("\nüìä M√âTRICAS REGRESI√ìN LOG√çSTICA:\n")
cat("Accuracy:", round(accuracy_logit, 4), "\n")
cat("Sensibilidad (Recall):", round(sensitivity_logit, 4), "\n")
cat("Especificidad:", round(specificity_logit, 4), "\n")
```

\newpage

## Random Forest

Random Forest es un m√©todo de **ensamble no lineal** que construye m√∫ltiples √°rboles de decisi√≥n mediante bootstrap y promedia sus predicciones.

**Justificaci√≥n:** Incluimos Random Forest para capturar posibles **relaciones no lineales** entre las caracter√≠sticas y la variable respuesta que los m√©todos lineales (LDA, Log√≠stica) no pueden modelar.

```{r random_forest}
# Crear dataframe para Random Forest
train_data_rf <- data.frame(Diagnosis = y_train, X_train_scaled)

# Entrenar modelo Random Forest con par√°metros por defecto
set.seed(123)
rf_model <- randomForest(Diagnosis ~ ., data = train_data_rf,
                         ntree = 500,
                         importance = TRUE)

# Mostrar resumen del modelo
print(rf_model)

# Predecir en datos de TEST
test_data_rf <- data.frame(X_test_scaled)
predictions_rf <- predict(rf_model, newdata = test_data_rf)

# Matriz de confusi√≥n
conf_matrix_rf <- confusionMatrix(predictions_rf, y_test, positive = "M")

print(conf_matrix_rf$table)

# Extraer m√©tricas
accuracy_rf <- conf_matrix_rf$overall["Accuracy"]
sensitivity_rf <- conf_matrix_rf$byClass["Sensitivity"]
specificity_rf <- conf_matrix_rf$byClass["Specificity"]

cat("\nüìä M√âTRICAS RANDOM FOREST:\n")
cat("Accuracy:", round(accuracy_rf, 4), "\n")
cat("Sensibilidad (Recall):", round(sensitivity_rf, 4), "\n")
cat("Especificidad:", round(specificity_rf, 4), "\n")
```

### Importancia de Variables (Random Forest)

```{r rf_importance, fig.cap="Top 10 Variables M√°s Importantes (Random Forest)"}
# Extraer importancia de variables
importance_df <- data.frame(
  Variable = rownames(rf_model$importance),
  MeanDecreaseGini = rf_model$importance[, "MeanDecreaseGini"]
)

# Ordenar y seleccionar top 10
importance_df <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(10)

ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de Variables en Random Forest",
       subtitle = "Top 10 Variables",
       x = "Variable",
       y = "Mean Decrease Gini") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))
```

\newpage

# Comparaci√≥n Final de Modelos

Evaluamos los tres modelos sobre el **conjunto de prueba independiente** para garantizar una evaluaci√≥n justa.

```{r final_comparison}
# Crear tabla comparativa
comparison_table <- data.frame(
  Modelo = c("LDA Manual", "Regresi√≥n Log√≠stica", "Random Forest"),
  Accuracy = c(accuracy_lda, accuracy_logit, accuracy_rf),
  Sensibilidad = c(sensitivity_lda, sensitivity_logit, sensitivity_rf),
  Especificidad = c(specificity_lda, specificity_logit, specificity_rf)
)

comparison_table %>%
  kable(caption = "Comparaci√≥n de Modelos de Clasificaci√≥n (Test Set)",
        digits = 4,
        booktabs = TRUE,
        col.names = c("Modelo", "Accuracy", "Sensibilidad (Recall)", "Especificidad")) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  row_spec(which.max(comparison_table$Accuracy), bold = TRUE, background = "#D5E8D4")
```

## Visualizaci√≥n Comparativa

```{r comparison_plot, fig.cap="Comparaci√≥n de M√©tricas entre Modelos"}
# Transformar datos para ggplot
comparison_long <- comparison_table %>%
  pivot_longer(cols = c(Accuracy, Sensibilidad, Especificidad),
               names_to = "Metrica",
               values_to = "Valor")

ggplot(comparison_long, aes(x = Modelo, y = Valor, fill = Metrica)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Comparaci√≥n de Desempe√±o de Modelos",
       subtitle = "Evaluaci√≥n en Conjunto de Prueba",
       x = "Modelo",
       y = "Valor de M√©trica") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")
```

\newpage

# Conclusiones

## Normalidad Multivariada

Los tests de Mardia y Royston indican que los datos **no cumplen estrictamente** con el supuesto de normalidad multivariada. Sin embargo, LDA es robusto ante desviaciones moderadas, especialmente con muestras grandes ($n > 300$).

## Estructuras de Covarianza

1. **PCA**: Los primeros 3-4 componentes principales capturan >90% de la varianza. La implementaci√≥n manual coincide exactamente con `prcomp()`.

2. **Factor Analysis**: Identificamos 5 factores latentes que representan diferentes aspectos de la morfolog√≠a tumoral (tama√±o, textura, forma, etc.).

3. **CCA**: Las variables "Mean" y "Worst" est√°n altamente correlacionadas ($r > 0.9$), lo cual es esperado.

## Comparaci√≥n de Modelos

```{r best_model, echo=FALSE}
best_model_idx <- which.max(comparison_table$Accuracy)
best_model_name <- comparison_table$Modelo[best_model_idx]
best_accuracy <- comparison_table$Accuracy[best_model_idx]
```

**üèÜ Modelo Ganador: `r best_model_name`**

- **Accuracy:** `r round(best_accuracy, 4)`
- **Sensibilidad:** `r round(comparison_table$Sensibilidad[best_model_idx], 4)`
- **Especificidad:** `r round(comparison_table$Especificidad[best_model_idx], 4)`

### An√°lisis por Modelo:

1. **LDA Manual**:
   - Implementaci√≥n exitosa desde cero usando √°lgebra matricial
   - Desempe√±o competitivo a pesar de la violaci√≥n de normalidad
   - Altamente interpretable

2. **Regresi√≥n Log√≠stica**:
   - Modelo lineal simple y robusto
   - Desempe√±o similar a LDA (esperado, ya que ambos son lineales)
   - Permite interpretaci√≥n de coeficientes

3. **Random Forest**:
   - Captura relaciones no lineales
   - Generalmente el mejor desempe√±o en accuracy
   - Menos interpretable pero m√°s flexible

## Recomendaci√≥n Final

Para **diagn√≥stico cl√≠nico**, se recomienda:

- **Random Forest** por su alta accuracy y capacidad de manejar interacciones complejas
- **LDA** como modelo alternativo interpretable para entender la estructura discriminante

Para **investigaci√≥n acad√©mica y cumplimiento de requisitos**:

- ‚úÖ Implementaci√≥n manual de PCA y LDA con √°lgebra matricial
- ‚úÖ Validaci√≥n de supuestos mediante tests de normalidad multivariada
- ‚úÖ Comparaci√≥n exhaustiva de modelos supervisados
- ‚úÖ Visualizaciones profesionales y tablas explicativas

---

## Referencias

- UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set
- Mardia, K. V. (1970). Measures of multivariate skewness and kurtosis
- Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32

---

**Fecha de generaci√≥n:** `r Sys.Date()`

**Nota:** Este documento fue generado con R Markdown y contiene c√≥digo completamente reproducible.
